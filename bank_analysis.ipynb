{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d48e527-8f26-4fd7-854c-dcd4177c56f7",
   "metadata": {},
   "source": [
    "## <center> Bank Marketing Analysis </center> \n",
    "\n",
    "<center>by Runtian Li, Rafe Chang, Sid Grover, Anu Banga </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728152f8-61fb-412a-b81b-8c3c55bbb128",
   "metadata": {},
   "source": [
    "**Repo Link:** https://github.com/UBC-MDS/dsci_522_group_8.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86a7598-c264-44e7-b70b-4495c8e01e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary Packages\n",
    "import altair as alt\n",
    "import altair_viewer\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statistics\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append(\"code/.\")\n",
    "\n",
    "# Data \n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# Machine Learning\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "from IPython.display import HTML, display\n",
    "# from plotting_functions import *\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score, cross_validate, train_test_split\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.metrics import make_scorer, f1_score, recall_score, precision_score, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "# %matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d925b92-d8fd-4fb3-8553-4810d94874ba",
   "metadata": {},
   "source": [
    "## <center> Summary </center>\n",
    "Here we build a model of balanced SVC to try to predict if a new client will subscribe to a term deposit. We tested five different classification models, including dummy classifier, unbalanced/balanced logistic regression, and unbalanced/balanced SVC, and chose the optimal model of balanced SVC based on how the model scored on the test data; the model has the highest test recall score of 0.82, which indicates that the model makes the least false negative predictions among all five models. \n",
    "\n",
    "The balanced support vector machines model considers 13 different numerical/ categorical features of customers. After hyperparameter optimization, the model's test accuracy increased from 0.82 to 0.875. The results were somewhat expected, given SVC's known efficacy in classification tasks, particularly when there's a clear margin of separation. The high recall score of 0.875 indicates that the model is particularly adept at identifying clients likely to subscribe, which was the primary goal. It's noteworthy that such a high recall was achieved, as it suggests the model is highly sensitive to true positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f5fbb-5151-4750-8afc-5544e88f896d",
   "metadata": {},
   "source": [
    "## <center> Introduction </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91a668-5c7b-4716-8ca3-e0d07d20842d",
   "metadata": {},
   "source": [
    "### Background\n",
    "The data set Bank Marketing was created by SeÃÅrgio Moro and Paulo Rita at the University Institute of Lisbon, and Paulo Cortez at the University of Minhom. It is sourced from the UCI Machine Learning Repository. Each row in this data set is an observation related to direct marketing campaigns (phone calls) of a Portuguese banking institution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6d1ab-c025-4420-a371-8f58ad9f82fa",
   "metadata": {},
   "source": [
    "### Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2428b-4782-4f39-9e8a-8db5dd1654db",
   "metadata": {},
   "source": [
    "We are working on a binary classification model. The classification goal is to predict if the client will subscribe a term deposit: \"yes\" for will subscribe and \"no\" for won't subscribe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2641d1-3399-4d8f-be42-336838cc1d86",
   "metadata": {},
   "source": [
    "### Data Description\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. It was sourced from the UCI Machine Learning Repository and can be found [here](https://archive.ics.uci.edu/dataset/222/bank+marketing). We will be using bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs). \n",
    "\n",
    "These are the detail of all inputs:\n",
    "\n",
    "| Feature Name | Type        | Description                                                                                   | Classes |\n",
    "|--------------|-------------|-----------------------------------------------------------------------------------------------|---------|\n",
    "| age          | Numeric     |                                                                                               |         |\n",
    "| job          | Categorical | Type of job                                                                                   | 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown' |\n",
    "| marital      | Categorical | Marital status                                                                                | 'divorced','married','single','unknown' |\n",
    "| education    | Categorical |                                                                                               | 'primary', 'secondary', 'tertiary', 'unknown' |\n",
    "| default      | Categorical | Has credit in default?                                                                        | 'no', 'yes', 'unknown' |\n",
    "| housing      | Categorical | Has housing loan?                                                                             | 'no', 'yes', 'unknown' |\n",
    "| loan         | Categorical | Has personal loan?                                                                            | 'no', 'yes', 'unknown' |\n",
    "| balance      | Numeric     | Balance of the individual                                                                     |         |\n",
    "| contact      | Categorical | Contact communication type                                                                    | 'cellular', 'telephone' |\n",
    "| month        | Categorical | Last contact month of year                                                                    | 'jan', 'feb', 'mar', ..., 'nov', 'dec' |\n",
    "| day          | Categorical | Last contact day of the week                                                                  | 'mon', 'tue', 'wed', 'thu', 'fri' |\n",
    "| duration     | Numeric     | Last contact duration, in seconds                                                             |         |\n",
    "| campaign     | Numeric     | Number of contacts performed during this campaign and for this client                        |         |\n",
    "| pdays        | Numeric     | Number of days that passed by after the client was last contacted from a previous campaign    |         |\n",
    "| previous     | Numeric     | Number of contacts performed before this campaign and for this client                         |         |\n",
    "| poutcome     | Categorical | Outcome of the previous marketing campaign                                                    | 'failure', 'nonexistent', 'success' |\n",
    "| y            | Binary      | Has the client subscribed to a term deposit?                                                  | 'yes', 'no' |\n",
    "\n",
    "\n",
    "The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9eb913-a7f8-4d1f-80bf-5c4f7ac367dc",
   "metadata": {},
   "source": [
    "## <center>Results and Discussion </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7db26c-39f9-4a3a-84ef-4012c2080629",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9b26b4-525d-47b5-8835-5876b3bd899e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 0     58\n",
       " 1     44\n",
       " 2     33\n",
       " 3     47\n",
       " 4     35\n",
       "       ..\n",
       " 72    84\n",
       " 73    87\n",
       " 74    92\n",
       " 75    93\n",
       " 76    88\n",
       " Length: 77, dtype: int64,\n",
       " 'job': 0        management\n",
       " 1        technician\n",
       " 2      entrepreneur\n",
       " 3       blue-collar\n",
       " 4           unknown\n",
       " 5           retired\n",
       " 6            admin.\n",
       " 7          services\n",
       " 8     self-employed\n",
       " 9        unemployed\n",
       " 10        housemaid\n",
       " 11          student\n",
       " dtype: object,\n",
       " 'marital': 0     married\n",
       " 1      single\n",
       " 2    divorced\n",
       " dtype: object,\n",
       " 'education': 0     tertiary\n",
       " 1    secondary\n",
       " 2      unknown\n",
       " 3      primary\n",
       " dtype: object,\n",
       " 'default': 0     no\n",
       " 1    yes\n",
       " dtype: object,\n",
       " 'housing': 0    yes\n",
       " 1     no\n",
       " dtype: object,\n",
       " 'loan': 0     no\n",
       " 1    yes\n",
       " dtype: object,\n",
       " 'contact': 0      unknown\n",
       " 1     cellular\n",
       " 2    telephone\n",
       " dtype: object,\n",
       " 'day': 0      5\n",
       " 1      6\n",
       " 2      7\n",
       " 3      8\n",
       " 4      9\n",
       " 5     12\n",
       " 6     13\n",
       " 7     14\n",
       " 8     15\n",
       " 9     16\n",
       " 10    19\n",
       " 11    20\n",
       " 12    21\n",
       " 13    23\n",
       " 14    26\n",
       " 15    27\n",
       " 16    28\n",
       " 17    29\n",
       " 18    30\n",
       " 19     2\n",
       " 20     3\n",
       " 21     4\n",
       " 22    11\n",
       " 23    17\n",
       " 24    18\n",
       " 25    24\n",
       " 26    25\n",
       " 27     1\n",
       " 28    10\n",
       " 29    22\n",
       " 30    31\n",
       " dtype: int64,\n",
       " 'month': 0     may\n",
       " 1     jun\n",
       " 2     jul\n",
       " 3     aug\n",
       " 4     oct\n",
       " 5     nov\n",
       " 6     dec\n",
       " 7     jan\n",
       " 8     feb\n",
       " 9     mar\n",
       " 10    apr\n",
       " 11    sep\n",
       " dtype: object,\n",
       " 'duration': 0        261\n",
       " 1        151\n",
       " 2         76\n",
       " 3         92\n",
       " 4        198\n",
       "         ... \n",
       " 1568    1440\n",
       " 1569    1405\n",
       " 1570    1298\n",
       " 1571    1246\n",
       " 1572    1556\n",
       " Length: 1573, dtype: int64,\n",
       " 'campaign': 0      1\n",
       " 1      2\n",
       " 2      3\n",
       " 3      5\n",
       " 4      4\n",
       " 5      6\n",
       " 6      7\n",
       " 7      8\n",
       " 8      9\n",
       " 9     10\n",
       " 10    11\n",
       " 11    12\n",
       " 12    13\n",
       " 13    19\n",
       " 14    14\n",
       " 15    24\n",
       " 16    16\n",
       " 17    32\n",
       " 18    18\n",
       " 19    22\n",
       " 20    15\n",
       " 21    17\n",
       " 22    25\n",
       " 23    21\n",
       " 24    43\n",
       " 25    51\n",
       " 26    63\n",
       " 27    41\n",
       " 28    26\n",
       " 29    28\n",
       " 30    55\n",
       " 31    50\n",
       " 32    38\n",
       " 33    23\n",
       " 34    20\n",
       " 35    29\n",
       " 36    31\n",
       " 37    37\n",
       " 38    30\n",
       " 39    46\n",
       " 40    27\n",
       " 41    58\n",
       " 42    33\n",
       " 43    35\n",
       " 44    34\n",
       " 45    36\n",
       " 46    39\n",
       " 47    44\n",
       " dtype: int64,\n",
       " 'pdays': 0       -1\n",
       " 1      151\n",
       " 2      166\n",
       " 3       91\n",
       " 4       86\n",
       "       ... \n",
       " 554    541\n",
       " 555    543\n",
       " 556    871\n",
       " 557    550\n",
       " 558    530\n",
       " Length: 559, dtype: int64,\n",
       " 'previous': 0       0\n",
       " 1       3\n",
       " 2       1\n",
       " 3       4\n",
       " 4       2\n",
       " 5      11\n",
       " 6      16\n",
       " 7       6\n",
       " 8       5\n",
       " 9      10\n",
       " 10     12\n",
       " 11      7\n",
       " 12     18\n",
       " 13      9\n",
       " 14     21\n",
       " 15      8\n",
       " 16     14\n",
       " 17     15\n",
       " 18     26\n",
       " 19     37\n",
       " 20     13\n",
       " 21     25\n",
       " 22     20\n",
       " 23     27\n",
       " 24     17\n",
       " 25     23\n",
       " 26     38\n",
       " 27     29\n",
       " 28     24\n",
       " 29     51\n",
       " 30    275\n",
       " 31     22\n",
       " 32     19\n",
       " 33     30\n",
       " 34     58\n",
       " 35     28\n",
       " 36     32\n",
       " 37     40\n",
       " 38     55\n",
       " 39     35\n",
       " 40     41\n",
       " dtype: int64,\n",
       " 'poutcome': 0    unknown\n",
       " 1    failure\n",
       " 2      other\n",
       " 3    success\n",
       " dtype: object,\n",
       " 'target': 0     no\n",
       " 1    yes\n",
       " dtype: object}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Import the unique feature values functions function from the src folder\n",
    "sys.path.append('..')\n",
    "from src.unique import get_uniques\n",
    "\n",
    "df = pd.read_csv(\"data/bank-full.csv\", delimiter=\";\")\n",
    "df.rename(columns={\"y\": \"target\"}, inplace=True)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=123)\n",
    "    \n",
    "get_uniques(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81e9a98-137b-475e-b1ba-09db87e3474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the eda_plotting functions function from the src folder\n",
    "sys.path.append('..')\n",
    "from src.eda_plotting import (\n",
    "                                EDA_plot, \n",
    "                                spearman_correlation_matrix, \n",
    "                                text_EDA\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338b68f-1bc2-4050-acc1-70a8be90a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = train_df.select_dtypes(include=['int64', 'float64']).columns.to_list()\n",
    "categorical_cols = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"poutcome\"]\n",
    "numerical_cols = numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2480258-aadb-41dd-a165-412ab3f71ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_EDA(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb356cd-fa3d-4a07-875c-0fd6e819283f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spearman_correlation_matrix(df, numerical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dae93a-037b-47e3-b74f-6cdc23859812",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(EDA_plot(df, numeric_cols, categorical_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e6ecc-a2d8-4287-b978-e33b5d1cede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d2b41-c903-427d-bbd9-c1332d284c73",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e616d5c-03d6-422b-85a1-c9dc860d5683",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "- Since there is no missing values in our dataset, we don't need to do imputation or drop NAs.   \n",
    "- We are going to drop \"contact\", \"day\" and \"month\" column here since they are not helping us in identifying useful underlying pattern in the model.    \n",
    "- We take \"age\", \"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\" as numerical features and we are doing StandardScaler transformation on them.\n",
    "- We take \"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"poutcome\" as categorical features and we are doing one hot encoding on them. We dropped columns only if the categorical is binary.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e677d9a0-c2ad-427c-8c64-77560bdcba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_looking_columns = train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "print(numeric_looking_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76daa040-dbf0-408a-b4c6-007c13a6be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists of feature names\n",
    "numerical_features = [\"age\", \"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "categorical_features = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"poutcome\"]\n",
    "drop_features = [\"contact\", \"day\", \"month\"]\n",
    "\n",
    "# Create the column transformer\n",
    "preprocessor = make_column_transformer(    \n",
    "    (StandardScaler(), numerical_features),  # scaling on numeric features   \n",
    "    (OneHotEncoder(drop=\"if_binary\"), categorical_features),  # OHE on categorical features\n",
    "    (\"drop\", drop_features),  # drop the drop features\n",
    ")\n",
    "\n",
    "# Show the preprocessor\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910d3a81-b522-42c4-8055-93bcec3557c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate X and y\n",
    "X_train = train_df.drop(columns=[\"target\"])\n",
    "X_test = test_df.drop(columns=[\"target\"])\n",
    "y_train = train_df[\"target\"]\n",
    "y_test = test_df[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc33905-92d6-49e0-b81b-639e31aeeaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This line nicely formats the feature names from `preprocessor.get_feature_names_out()` so that we can more easily use them below\n",
    "preprocessor.verbose_feature_names_out = False\n",
    "\n",
    "# Create a dataframe with the transformed features and column names\n",
    "ct = preprocessor.fit(X_train)\n",
    "\n",
    "# Columns names after one hot encoding\n",
    "ohe_columns = list(\n",
    "    preprocessor.named_transformers_[\"onehotencoder\"]\n",
    "    .get_feature_names_out(categorical_features)\n",
    ")\n",
    "\n",
    "# Columns after transformation\n",
    "new_columns = (\n",
    "    numerical_features + ohe_columns\n",
    ")\n",
    "\n",
    "# Now create the DataFrame with the dense data\n",
    "X_train_enc = pd.DataFrame(preprocessor.transform(X_train), index=X_train.index, columns=new_columns)\n",
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6060eb-8aaa-46ca-943d-0b4770c1d325",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a31a6cd-5abd-4a9d-ab2d-c7fc6287dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Base Model: Dummy Classifier\n",
    "classification_metrics = [\"accuracy\", \"precision\", \"recall\", \"f1\"]\n",
    "dc = DummyClassifier(strategy=\"most_frequent\")\n",
    "pipe_dc = make_pipeline(preprocessor, dc)\n",
    "# The mean and std of the cross validated scores for all metrics as a dataframe\n",
    "cross_val_results = {}\n",
    "scoring = {\n",
    "    \"accuracy\": 'accuracy',\n",
    "    'precision': make_scorer(precision_score, pos_label=\"yes\", zero_division=0),\n",
    "    'recall': make_scorer(recall_score, pos_label=\"yes\"),\n",
    "    'f1': make_scorer(f1_score, pos_label=\"yes\")\n",
    "}  # scoring can be a string, a list, or a dictionary\n",
    "\n",
    "cross_val_results['dummy'] = pd.DataFrame(cross_validate(pipe_dc, X_train, y_train, return_train_score=True, scoring=scoring)).agg(['mean', 'std']).round(3).T\n",
    "\n",
    "# Show the train and validation scores\n",
    "cross_val_results['dummy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4993a2-6805-4c53-b26f-674456bc7b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Logistic regression\n",
    "\n",
    "# The logreg model pipeline\n",
    "logreg = make_pipeline(preprocessor, LogisticRegression(max_iter=1000, random_state=123))\n",
    "\n",
    "# The mean and std of the cross validated scores for all metrics as a dataframe\n",
    "cross_val_results['logreg'] = pd.DataFrame(cross_validate(logreg, X_train, y_train, return_train_score=True, scoring=scoring)).agg(['mean', 'std']).round(3).T\n",
    "\n",
    "# Show the train and validation scores\n",
    "cross_val_results['logreg'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9645fc-26e5-42da-b28c-2e88d7c1016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Support vector classifier\n",
    "\n",
    "# The svc model pipeline\n",
    "svc = make_pipeline(preprocessor, SVC(random_state=123))\n",
    "\n",
    "# The mean and std of the cross validated scores for all metrics as a dataframe\n",
    "cross_val_results['svc'] = pd.DataFrame(cross_validate(svc, X_train, y_train, return_train_score=True, scoring=scoring)).agg(['mean', 'std']).round(3).T\n",
    "# Show the train and validation scores\n",
    "cross_val_results['svc'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81c85e8-33ff-469b-901b-35d76f9cc4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Balanced logistic regression\n",
    "logreg_bal = make_pipeline(preprocessor, \n",
    "                           LogisticRegression(max_iter=1000, \n",
    "                                              random_state=123, \n",
    "                                              class_weight=\"balanced\"))\n",
    "\n",
    "# The mean and std of the cross validated scores for all metrics as a dataframe\n",
    "cross_val_results['logreg_bal'] = pd.DataFrame(cross_validate(logreg_bal, X_train, y_train, return_train_score=True, scoring=scoring)).agg(['mean', 'std']).round(3).T\n",
    "\n",
    "# Show the train and validation scores\n",
    "cross_val_results['logreg_bal'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2b450-b074-4a2b-9e7b-821469be6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Balanced support vector classifier\n",
    "svc_bal = make_pipeline(preprocessor, SVC(random_state=123, class_weight=\"balanced\"))\n",
    "\n",
    "# The mean and std of the cross validated scores for all metrics as a dataframe\n",
    "cross_val_results['svc_bal'] = pd.DataFrame(cross_validate(svc_bal, X_train, y_train, return_train_score=True, scoring=scoring)).agg(['mean', 'std']).round(3).T\n",
    "\n",
    "# Show the train and validation scores\n",
    "cross_val_results['svc_bal'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fbf4a9-2252-4e23-99c5-86673a4963ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the average scores of all the models\n",
    "pd.concat(\n",
    "    cross_val_results,\n",
    "    axis='columns'\n",
    ").xs(\n",
    "    'mean',\n",
    "    axis='columns',\n",
    "    \n",
    "    level=1\n",
    ").style.format(\n",
    "    precision=2\n",
    ").background_gradient(\n",
    "    axis=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa13c9-44ee-4549-b3cb-055a41ec69be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "`Dummy Classifier` has low accuracy and zero precision, recall, and F1 scores, indicating it never predicts the positive class (in this case the client subscribed a term deposit). This is expected as it always predicts the most frequent class.\n",
    "\n",
    "`logreg` shows improved accuracy over the dummy model. However, its recall is low, suggesting it misses a significant number of true positive cases. `svc` performed almost the same as logistic regression model among all metrics.\n",
    "\n",
    "`logreg_bal` and `svc_bal` have lower accuracy compared to their unbalanced counterparts but significantly higher recall. This indicates they are better at identifying positive cases but at the cost of making more false positive errors.\n",
    "\n",
    "Given the context of our bank marketing data set, we aim to detect the clients who will subscribe a term deposit given the features. Missing a potential \"yes\" could be more costly than false positives, as it represents a lost opportunity for the sales team to transform this potential customer. Therefore, we chose `svc_bal` as the model has the highest `test_recall` score. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75977c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_bal.fit(X_train, y_train)\n",
    "confmat_svc_bal = ConfusionMatrixDisplay.from_estimator(\n",
    "    svc_bal,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    values_format=\"d\",) \n",
    "confmat_svc_bal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b52bfd-07b7-43b9-b743-01aabd24775c",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017df7dc-d874-4fb2-af05-8e070dd819c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Optimizing hyperparameters in SVC with a smaller sample size of 10,000 instances is a strategy aimed at enhancing computational efficiency. This approach expedites the exploration of hyperparameter possibilities, aiding in the discovery of potential configurations. While the outcomes validate the concept, it's crucial to recognize and manage the constraints stemming from the smaller dataset size when interpreting the results.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5cffe6-a26f-4822-abb2-10cc9fb9b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a sample of 10000 observations\n",
    "sample_data = df.sample(n=10000, random_state=123)\n",
    "train_df_sampled, test_df_sampled = train_test_split(sample_data, test_size=0.2, random_state=123)\n",
    "\n",
    "X_train_sampled = train_df_sampled.drop(columns=[\"target\"])\n",
    "X_test_sampled = test_df_sampled.drop(columns=[\"target\"])\n",
    "y_train_sampled = train_df_sampled[\"target\"]\n",
    "y_test_sampled = test_df_sampled[\"target\"]\n",
    "\n",
    "# Transformation on the sample training data\n",
    "sample_preprocessor = make_column_transformer(\n",
    "    (StandardScaler(), numerical_features),\n",
    "    (OneHotEncoder(drop=\"if_binary\"), categorical_features),\n",
    "    (\"drop\", drop_features),\n",
    ")\n",
    "\n",
    "X_train_sampled_enc = pd.DataFrame(sample_preprocessor.fit_transform(X_train_sampled), index=X_train_sampled.index, columns=new_columns)\n",
    "\n",
    "svc_bal_sample = make_pipeline(sample_preprocessor, SVC(random_state=123, class_weight=\"balanced\"))\n",
    "\n",
    "param_dist = {\n",
    "    'svc__C': uniform(0.1, 10),\n",
    "    'svc__gamma': uniform(0.001, 0.1),\n",
    "    'svc__kernel': ['rbf', 'sigmoid', 'linear']\n",
    "}\n",
    "\n",
    "# Perform RandomizedSearchCV for hyperparameter optimization\n",
    "random_search = RandomizedSearchCV(svc_bal_sample, param_distributions=param_dist, n_iter=25, cv=5, n_jobs=-1, random_state=123)\n",
    "random_search.fit(X_train_sampled, y_train_sampled)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params_random = random_search.best_params_\n",
    "print(\"Best Hyperparameters (Randomized Search):\", best_params_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2745b0e-b4d2-4fc8-bca5-64f69fd7fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(random_search.cv_results_)[\n",
    "    [\n",
    "        \"mean_test_score\",\n",
    "        \"param_svc__gamma\",\n",
    "        \"param_svc__C\",\n",
    "        \"mean_fit_time\",\n",
    "        \"rank_test_score\",\n",
    "    ]\n",
    "].set_index(\"rank_test_score\").sort_index().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6db8d0-61a2-461d-b8cf-9f1d2fda19b7",
   "metadata": {},
   "source": [
    "### Test results after hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b84448-68b9-4cfe-9efa-87e251a8e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the test set\n",
    "best_model_random = random_search.best_estimator_\n",
    "accuracy_random = best_model_random.score(X_test, y_test)\n",
    "print(\"Accuracy on Test Set:\", accuracy_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f77e6f-f739-4baa-9fad-dfa9a6984828",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_model_random.predict(X_test)\n",
    "\n",
    "recall = recall_score(y_test, predictions, pos_label='yes')\n",
    "print(\"Recall on Test Set:\", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fbec9b-254d-4e69-b824-a687193772c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(random_search.cv_results_)\n",
    "\n",
    "scatter = alt.Chart(results).mark_circle().encode(\n",
    "    x='param_svc__C:Q',\n",
    "    y='param_svc__gamma:Q',\n",
    "    color=alt.Color('mean_test_score:Q', \n",
    "                    scale=alt.Scale(scheme='viridis', reverse=True)\n",
    "                   )\n",
    ").properties(\n",
    "    width=400,\n",
    "    height=300,\n",
    "    title='C and gamma vs. Mean Test Score'\n",
    ")\n",
    "\n",
    "scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df689c-62be-4c95-86a9-2e82533edb52",
   "metadata": {},
   "source": [
    "# <center> Discussions </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a76f22-f7f8-4edd-870b-abd1e59dd7e4",
   "metadata": {},
   "source": [
    "### Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace47a8-cebe-4470-a797-2e5d959855e3",
   "metadata": {},
   "source": [
    "In this bank marketing analysis project, we aimed to develop a binary classification model to predict client subscription to term deposits. We tested Logistic Regression and Support Vector Classifier (SVC) models, focusing on recall as a key performance metric. The SVC model outperformed Logistic Regression in recall, and after hyperparameter optimization, it achieved a recall score of 0.875 on the test dataset, which is quite promising!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aaa939-7934-43db-8cdc-7921addbd44c",
   "metadata": {},
   "source": [
    "### Reflection on Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87f4c4-7555-4393-bbcd-065cfd433668",
   "metadata": {},
   "source": [
    "The results were somewhat expected, given SVC's known efficacy in classification tasks, particularly when there's a clear margin of separation. The high recall score of 0.875 indicates that the model is particularly adept at identifying clients likely to subscribe, which was the primary goal. It's noteworthy that such a high recall was achieved, as it suggests the model is highly sensitive to true positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b897f2-0271-4d04-ae7b-e4ff0bf99ab3",
   "metadata": {},
   "source": [
    "### Impact of Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c65dd6-5f02-4742-853f-d53397dc7819",
   "metadata": {},
   "source": [
    "The high recall score of this model has significant implications for targeted marketing strategies. It suggests that the bank can confidently use the model's predictions to focus its marketing efforts on clients predicted to subscribe, potentially increasing the efficiency and effectiveness of its campaigns. This targeted approach could lead to higher conversion rates with lower marketing expenses. However, it's important to balance such a high recall with precision to ensure that the bank doesn't unnecessarily target unlikely prospects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6af611-4a94-4b9f-a1fc-6beec5f8a08e",
   "metadata": {},
   "source": [
    "### Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96100f-1bcb-45b9-8ef2-5c1433a84825",
   "metadata": {},
   "source": [
    "The success of this model leads to several potential areas for further exploration:\n",
    "\n",
    "- Balancing Precision and Recall: Investigating methods to enhance precision without substantially reducing recall.\n",
    "- Feature Analysis: Identifying which features most significantly influence subscription predictions.\n",
    "Model Interpretability: Improving the model's interpretability to better understand the basis for its predictions.\n",
    "- Temporal Adaptability: Assessing the model's adaptability to evolving trends and customer behaviors over time.\n",
    "- Testing Alternative Models: Exploring whether ensemble methods or more advanced machine learning algorithms could yield better or comparable results.\n",
    "- Customer Segmentation: Evaluating the model's performance across different customer segments to tailor more specific marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b6e27-a5e9-4a01-84c4-9aea31a097a7",
   "metadata": {},
   "source": [
    "# <center> References </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb20d746-d653-4366-b521-8c9a1b485b23",
   "metadata": {},
   "source": [
    "Moro,S., Rita,P., and Cortez,P., 2012. Bank Marketing. UCI Machine Learning Repository. https://doi.org/10.24432/C5K306\"\n",
    "\n",
    "Timbers,T. , Ostblom,J., and Lee,M., 2023. Breast Cancer Predictor Report. GitHub repository, https://github.com/ttimbers/breast_cancer_predictor_py/blob/0.0.1/src/breast_cancer_predictor_report.ipynb\",\n",
    "\n",
    "Moro, S., Cortez, P., & Rita, P. (2014). A data-driven approach to predict the success of bank telemarketing. Decis. Support Syst., 62, 22-31.\n",
    "\n",
    "Alsolami, F.J., Saleem, F., & Al-Ghamdi, A.S. (2020). Predicting the Accuracy for Telemarketing Process in Banks Using Data Mining.\n",
    "\n",
    "Vajiramedhin, C., & Suebsing, A. (2014). Feature Selection with Data Balancing for Prediction of Bank Telemarketing. Applied mathematical sciences, 8, 5667-5672.\n",
    "\n",
    "Moura, A.F., Pinho, C.M., Napolitano, D.M., Martins, F.S., & Fornari Junior, J.C. (2020). Optimization of operational costs of Call centers employing classification techniques. Research, Society and Development, 9."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
