{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d48e527-8f26-4fd7-854c-dcd4177c56f7",
   "metadata": {},
   "source": [
    "# Bank Marketing Analysis\n",
    "\n",
    "by Runtian Li, Rafe Chang, Sid Grover, Anu Banga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728152f8-61fb-412a-b81b-8c3c55bbb128",
   "metadata": {},
   "source": [
    "**Repo Link:** https://github.com/UBC-MDS/dsci_522_group_8.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c86a7598-c264-44e7-b70b-4495c8e01e10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "## Import necessary Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from myst_nb import glue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d925b92-d8fd-4fb3-8553-4810d94874ba",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Here we build a model of balanced SVC to try to predict if a new client will subscribe to a term deposit. We tested five different classification models, including dummy classifier, unbalanced/balanced logistic regression, and unbalanced/balanced SVC, and chose the optimal model of balanced SVC based on how the model scored on the test data; the model has the highest test recall score of 0.82, which indicates that the model makes the least false negative predictions among all five models. \n",
    "\n",
    "The balanced support vector machines model considers 13 different numerical/ categorical features of customers. After hyperparameter optimization, the model's test accuracy increased from 0.82 to 0.875. The results were somewhat expected, given SVC's known efficacy in classification tasks, particularly when there's a clear margin of separation. The high recall score of 0.875 indicates that the model is particularly adept at identifying clients likely to subscribe, which was the primary goal. It's noteworthy that such a high recall was achieved, as it suggests the model is highly sensitive to true positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f5fbb-5151-4750-8afc-5544e88f896d",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14be05d4",
   "metadata": {},
   "source": [
    "Term deposit is valuable to banks because it ensures a stable stream of income that banks can utilize. Banks usually invest in higher-return financial products or lend money to other customers with a higher interest rate to make a profit. With term deposits, banks can better predict their cash flow.\n",
    "\n",
    "While banks's marketing strategies nowadays are usually focused on attracting new customers, the banks must target the right potential customers. This research is aimed at identifying the correct audience for banks to further design marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91a668-5c7b-4716-8ca3-e0d07d20842d",
   "metadata": {},
   "source": [
    "### Background\n",
    "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be ('yes') or not ('no') subscribed. The data set used in this project was created by Moro, S. and Rita, P. and Cortez, P {cite}`moro2012bank`.It was sourced from the UCI Machine Learning Repository {cite}`moro2012bank`. We will be using bank-full.csv with all examples and 17 inputs, ordered by date (older version of this dataset with less inputs). The raw data file can be found [here](https://archive.ics.uci.edu/dataset/222/bank+marketing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f6d1ab-c025-4420-a371-8f58ad9f82fa",
   "metadata": {},
   "source": [
    "### Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd2428b-4782-4f39-9e8a-8db5dd1654db",
   "metadata": {},
   "source": [
    "We are working on a binary classification model. The classification goal is to predict if the client will subscribe a term deposit: \"yes\" for will subscribe and \"no\" for won't subscribe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76d0d7-7668-4993-9d92-73498a60395a",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9720e91-b893-4c3e-a12d-d568b5a75a7c",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "Initially, we ensured our data was complete by dealing with missing values and removing unnecessary columns like \"contact,\" \"day,\" and \"month.\" This streamlined the dataset, making it ready for analysis. We used StandardScaler to standardize numerical features such as \"age\" and \"balance\" and applied one-hot encoding for categorical attributes, making the data compatible with different machine learning models.\n",
    "\n",
    "### Model Selection and Evaluation\n",
    "We used five models for classification, starting with a basic Dummy Classifier. Following models included Logistic Regression and Support Vector Classifier (SVC), each showing strengths in accuracy and recall {cite}`moro2014data`. Notably, our balanced models—Balanced Logistic Regression and Balanced Support Vector Classifier (svc_bal)—performed best, especially in identifying clients likely to subscribe to a term deposit.\n",
    "\n",
    "### Model Comparison\n",
    "An extensive evaluation, considering accuracy, precision, recall, and F1 scores, highlighted the Balanced Support Vector Classifier (svc_bal) as the standout performer. This model excelled with high recall, crucial for identifying potential term deposit subscribers in our specific context.\n",
    "\n",
    "### Hyperparameter Optimization\n",
    "Optimizing model performance, especially for the Support Vector Classifier (SVC) using a reduced dataset, resulted in a final model with an impressive 86% accuracy and a notable recall of 87%. This optimization strategy enhances efficiency and fine-tunes the model for better results.\n",
    "\n",
    "### Recall - The Preferred Metric\n",
    "In our bank marketing dataset, we prioritize recall. Recall indicates the model's ability to identify true positive cases—clients subscribing to a term deposit. In our context, missing a potential positive case is more significant than false positives, leading to potential losses and missed opportunities. Prioritizing recall ensures a finely tuned model capturing all potential clients interested in term deposits, aligning with our main goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9eb913-a7f8-4d1f-80bf-5c4f7ac367dc",
   "metadata": {},
   "source": [
    "## Modeling and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7db26c-39f9-4a3a-84ef-4012c2080629",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04592d9e-e431-447c-a11c-7aa589fd8cda",
   "metadata": {},
   "source": [
    "According to the discussion above, we decided to keep the following features as numerical features: \"age\", \"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\". See the distribution as below: {numref}`Figure {number} <numerical_features_distribution>`. {cite}`vajiramedhin2014feature`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea48490b-1e0a-418e-888e-7f41b5c06d01",
   "metadata": {},
   "source": [
    "```{figure} ../results/figures/numerical_dist_by_feat.png\n",
    "---\n",
    "width: 800px\n",
    "name: numerical_features_distribution\n",
    "---\n",
    "Distribution of all the numerical features after feature selection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d64b355-a271-43fd-bd4d-072c939984db",
   "metadata": {},
   "source": [
    "We decided to keep the following features as categorical features: \"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"poutcome\". See the distribution as below: {numref}`Figure {number} <categorical_features_distribution>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0e5a8-7e7b-473c-91bf-373727fd2f99",
   "metadata": {},
   "source": [
    "```{figure} ../results/figures/categorical_dist_by_feat.png\n",
    "---\n",
    "width: 800px\n",
    "name: categorical_features_distribution\n",
    "---\n",
    "Distribution of all the categorical features after feature selection\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de318ad1",
   "metadata": {},
   "source": [
    "In the plot below, we explore the spearman correlation between numerical features. {numref}`Figure {number} <corr_matx>`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e8159a",
   "metadata": {},
   "source": [
    "```{figure} ../results/figures/corr_matx.png\n",
    "---\n",
    "width: 800px\n",
    "name: corr_matx\n",
    "---\n",
    "Correlation Matrix of numerical features\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d2b41-c903-427d-bbd9-c1332d284c73",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e616d5c-03d6-422b-85a1-c9dc860d5683",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "- Since there is no missing values in our dataset, we don't need to do imputation or drop NAs.   \n",
    "- We are going to drop \"contact\", \"day\" and \"month\" column here since they are not helping us in identifying useful underlying pattern in the model.    \n",
    "- We take \"age\", \"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\" as numerical features and we are doing StandardScaler transformation on them.\n",
    "- We take \"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"poutcome\" as categorical features and we are doing one hot encoding on them. We dropped columns only if the categorical is binary.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1293c9fb-3054-481d-880d-36b8067aefa8",
   "metadata": {},
   "source": [
    "The transformed dataframe after doing `StandardScale` on numerical features and `OneHotEncoder` on categorical features is shown as below. The number of features after preprocessing is 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76daa040-dbf0-408a-b4c6-007c13a6be89",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Lists of feature names\n",
    "numerical_features = [\"age\", \"balance\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "categorical_features = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"poutcome\"]\n",
    "drop_features = [\"contact\", \"day\", \"month\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24215d7-50f8-4e79-9d21-1609caa74fc2",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>balance</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>job_admin.</th>\n",
       "      <th>job_blue-collar</th>\n",
       "      <th>job_entrepreneur</th>\n",
       "      <th>job_housemaid</th>\n",
       "      <th>...</th>\n",
       "      <th>education_secondary</th>\n",
       "      <th>education_tertiary</th>\n",
       "      <th>education_unknown</th>\n",
       "      <th>default_yes</th>\n",
       "      <th>housing_yes</th>\n",
       "      <th>loan_yes</th>\n",
       "      <th>poutcome_failure</th>\n",
       "      <th>poutcome_other</th>\n",
       "      <th>poutcome_success</th>\n",
       "      <th>poutcome_unknown</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.941018</td>\n",
       "      <td>-0.261980</td>\n",
       "      <td>0.584440</td>\n",
       "      <td>-0.246523</td>\n",
       "      <td>-0.413281</td>\n",
       "      <td>-0.300644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.175099</td>\n",
       "      <td>0.036315</td>\n",
       "      <td>-0.041580</td>\n",
       "      <td>-0.574519</td>\n",
       "      <td>-0.413281</td>\n",
       "      <td>-0.300644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.036757</td>\n",
       "      <td>-0.480295</td>\n",
       "      <td>-0.333467</td>\n",
       "      <td>1.721452</td>\n",
       "      <td>-0.413281</td>\n",
       "      <td>-0.300644</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.452477</td>\n",
       "      <td>-0.521486</td>\n",
       "      <td>2.900331</td>\n",
       "      <td>-0.246523</td>\n",
       "      <td>-0.413281</td>\n",
       "      <td>-0.300644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.079360</td>\n",
       "      <td>2.253443</td>\n",
       "      <td>1.821118</td>\n",
       "      <td>-0.574519</td>\n",
       "      <td>0.531537</td>\n",
       "      <td>0.205109</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9037</th>\n",
       "      <td>0.878038</td>\n",
       "      <td>0.779477</td>\n",
       "      <td>2.408732</td>\n",
       "      <td>0.081473</td>\n",
       "      <td>-0.413281</td>\n",
       "      <td>-0.300644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9038</th>\n",
       "      <td>-0.462319</td>\n",
       "      <td>-0.263696</td>\n",
       "      <td>0.384728</td>\n",
       "      <td>-0.574519</td>\n",
       "      <td>-0.413281</td>\n",
       "      <td>-0.300644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9039</th>\n",
       "      <td>0.207860</td>\n",
       "      <td>0.776044</td>\n",
       "      <td>0.058276</td>\n",
       "      <td>-0.574519</td>\n",
       "      <td>-0.413281</td>\n",
       "      <td>-0.300644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9040</th>\n",
       "      <td>1.356737</td>\n",
       "      <td>2.465236</td>\n",
       "      <td>-0.222089</td>\n",
       "      <td>-0.246523</td>\n",
       "      <td>-0.413281</td>\n",
       "      <td>-0.300644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9041</th>\n",
       "      <td>0.495079</td>\n",
       "      <td>0.257719</td>\n",
       "      <td>-0.659919</td>\n",
       "      <td>0.409469</td>\n",
       "      <td>-0.413281</td>\n",
       "      <td>-0.300644</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9042 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           age   balance  duration  campaign     pdays  previous  job_admin.  \\\n",
       "0    -0.941018 -0.261980  0.584440 -0.246523 -0.413281 -0.300644         0.0   \n",
       "1    -0.175099  0.036315 -0.041580 -0.574519 -0.413281 -0.300644         0.0   \n",
       "2    -1.036757 -0.480295 -0.333467  1.721452 -0.413281 -0.300644         1.0   \n",
       "3     1.452477 -0.521486  2.900331 -0.246523 -0.413281 -0.300644         0.0   \n",
       "4    -0.079360  2.253443  1.821118 -0.574519  0.531537  0.205109         0.0   \n",
       "...        ...       ...       ...       ...       ...       ...         ...   \n",
       "9037  0.878038  0.779477  2.408732  0.081473 -0.413281 -0.300644         0.0   \n",
       "9038 -0.462319 -0.263696  0.384728 -0.574519 -0.413281 -0.300644         0.0   \n",
       "9039  0.207860  0.776044  0.058276 -0.574519 -0.413281 -0.300644         0.0   \n",
       "9040  1.356737  2.465236 -0.222089 -0.246523 -0.413281 -0.300644         0.0   \n",
       "9041  0.495079  0.257719 -0.659919  0.409469 -0.413281 -0.300644         0.0   \n",
       "\n",
       "      job_blue-collar  job_entrepreneur  job_housemaid  ...  \\\n",
       "0                 0.0               0.0            0.0  ...   \n",
       "1                 0.0               0.0            0.0  ...   \n",
       "2                 0.0               0.0            0.0  ...   \n",
       "3                 1.0               0.0            0.0  ...   \n",
       "4                 0.0               0.0            0.0  ...   \n",
       "...               ...               ...            ...  ...   \n",
       "9037              0.0               0.0            0.0  ...   \n",
       "9038              0.0               0.0            0.0  ...   \n",
       "9039              1.0               0.0            0.0  ...   \n",
       "9040              0.0               0.0            0.0  ...   \n",
       "9041              0.0               0.0            0.0  ...   \n",
       "\n",
       "      education_secondary  education_tertiary  education_unknown  default_yes  \\\n",
       "0                     1.0                 0.0                0.0          0.0   \n",
       "1                     0.0                 1.0                0.0          0.0   \n",
       "2                     1.0                 0.0                0.0          0.0   \n",
       "3                     0.0                 0.0                0.0          0.0   \n",
       "4                     0.0                 1.0                0.0          0.0   \n",
       "...                   ...                 ...                ...          ...   \n",
       "9037                  1.0                 0.0                0.0          0.0   \n",
       "9038                  0.0                 1.0                0.0          0.0   \n",
       "9039                  0.0                 0.0                0.0          0.0   \n",
       "9040                  0.0                 0.0                0.0          0.0   \n",
       "9041                  0.0                 1.0                0.0          0.0   \n",
       "\n",
       "      housing_yes  loan_yes  poutcome_failure  poutcome_other  \\\n",
       "0             1.0       0.0               0.0             0.0   \n",
       "1             0.0       0.0               0.0             0.0   \n",
       "2             0.0       0.0               0.0             0.0   \n",
       "3             1.0       0.0               0.0             0.0   \n",
       "4             0.0       0.0               0.0             0.0   \n",
       "...           ...       ...               ...             ...   \n",
       "9037          1.0       0.0               0.0             0.0   \n",
       "9038          1.0       0.0               0.0             0.0   \n",
       "9039          0.0       0.0               0.0             0.0   \n",
       "9040          0.0       0.0               0.0             0.0   \n",
       "9041          1.0       0.0               0.0             0.0   \n",
       "\n",
       "      poutcome_success  poutcome_unknown  \n",
       "0                  0.0               1.0  \n",
       "1                  0.0               1.0  \n",
       "2                  0.0               1.0  \n",
       "3                  0.0               1.0  \n",
       "4                  1.0               0.0  \n",
       "...                ...               ...  \n",
       "9037               0.0               1.0  \n",
       "9038               0.0               1.0  \n",
       "9039               0.0               1.0  \n",
       "9040               0.0               1.0  \n",
       "9041               0.0               1.0  \n",
       "\n",
       "[9042 rows x 32 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the transformed X_train\n",
    "X_train_enc = pd.read_csv(\"../data/processed/X_train_enc.csv\")\n",
    "X_train_enc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6060eb-8aaa-46ca-943d-0b4770c1d325",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91fbf4a9-2252-4e23-99c5-86673a4963ec",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>scoring</th>\n",
       "      <th>dummy</th>\n",
       "      <th>dummy.1</th>\n",
       "      <th>logreg</th>\n",
       "      <th>logreg.1</th>\n",
       "      <th>svc</th>\n",
       "      <th>svc.1</th>\n",
       "      <th>logreg_bal</th>\n",
       "      <th>logreg_bal.1</th>\n",
       "      <th>svc_bal</th>\n",
       "      <th>svc_bal.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "      <td>mean</td>\n",
       "      <td>std</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fit_time</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.012</td>\n",
       "      <td>1.495</td>\n",
       "      <td>0.722</td>\n",
       "      <td>17.859</td>\n",
       "      <td>0.319</td>\n",
       "      <td>1.394</td>\n",
       "      <td>0.238</td>\n",
       "      <td>30.267</td>\n",
       "      <td>0.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>score_time</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.044</td>\n",
       "      <td>2.164</td>\n",
       "      <td>0.111</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.009</td>\n",
       "      <td>3.583</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_accuracy</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.906</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.904</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.821</td>\n",
       "      <td>0.008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_accuracy</td>\n",
       "      <td>0.887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.907</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>test_precision</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.667</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_precision</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.669</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.754</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.393</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>test_recall</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.316</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.777</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_recall</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.377</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_f1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_f1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.523</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            scoring  dummy dummy.1 logreg logreg.1     svc  svc.1 logreg_bal  \\\n",
       "0               NaN   mean     std   mean      std    mean    std       mean   \n",
       "1          fit_time  0.176   0.012  1.495    0.722  17.859  0.319      1.394   \n",
       "2        score_time  0.136   0.019  0.176    0.044   2.164  0.111      0.156   \n",
       "3     test_accuracy  0.887     0.0  0.906    0.007   0.904  0.005      0.837   \n",
       "4    train_accuracy  0.887     0.0  0.907    0.001   0.916  0.001      0.839   \n",
       "5    test_precision    0.0     0.0  0.667    0.062   0.662  0.057      0.388   \n",
       "6   train_precision    0.0     0.0  0.669    0.007   0.754  0.009      0.393   \n",
       "7       test_recall    0.0     0.0   0.34    0.033   0.316  0.029      0.777   \n",
       "8      train_recall    0.0     0.0  0.344    0.005   0.377  0.009      0.785   \n",
       "9           test_f1    0.0     0.0   0.45    0.039   0.426  0.032      0.518   \n",
       "10         train_f1    0.0     0.0  0.454    0.006   0.503  0.009      0.523   \n",
       "\n",
       "   logreg_bal.1 svc_bal svc_bal.1  \n",
       "0           std    mean       std  \n",
       "1         0.238  30.267      0.39  \n",
       "2         0.009   3.583     0.072  \n",
       "3          0.01   0.821     0.008  \n",
       "4         0.001   0.843     0.002  \n",
       "5          0.02   0.365     0.011  \n",
       "6         0.003    0.41     0.003  \n",
       "7          0.05   0.794     0.036  \n",
       "8         0.006   0.892     0.003  \n",
       "9         0.025     0.5     0.013  \n",
       "10        0.003   0.562     0.003  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_df=pd.read_csv(\"../results/metrics/model_selection_scores.csv\")\n",
    "scoring_df = scoring_df.rename(columns={'Unnamed: 0': 'scoring'})\n",
    "scoring_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efa13c9-44ee-4549-b3cb-055a41ec69be",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "`Dummy Classifier` has low accuracy and zero precision, recall, and F1 scores, indicating it never predicts the positive class (in this case the client subscribed a term deposit). This is expected as it always predicts the most frequent class.\n",
    "\n",
    "`logreg` shows improved accuracy over the dummy model. However, its recall is low, suggesting it misses a significant number of true positive cases. `svc` performed almost the same as logistic regression model among all metrics.\n",
    "\n",
    "`logreg_bal` and `svc_bal` have lower accuracy compared to their unbalanced counterparts but significantly higher recall. This indicates they are better at identifying positive cases but at the cost of making more false positive errors.\n",
    "\n",
    "Given the context of our bank marketing data set, we aim to detect the clients who will subscribe a term deposit given the features. Missing a potential \"yes\" could be more costly than false positives, as it represents a lost opportunity for the sales team to transform this potential customer. Therefore, we chose `svc_bal` as the model has the highest `test_recall` score. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ff5e1d-b95b-4650-afd2-e493569d669c",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>train_precision</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>train_recall</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.842734</td>\n",
       "      <td>0.814261</td>\n",
       "      <td>0.409378</td>\n",
       "      <td>0.366394</td>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.786601</td>\n",
       "      <td>0.56084</td>\n",
       "      <td>0.499926</td>\n",
       "      <td>36.729734</td>\n",
       "      <td>186.583015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_accuracy  test_accuracy  train_precision  test_precision  \\\n",
       "0        0.842734       0.814261         0.409378        0.366394   \n",
       "\n",
       "   train_recall  test_recall  train_f1   test_f1   fit_time  score_time  \n",
       "0      0.890196     0.786601   0.56084  0.499926  36.729734  186.583015  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring_metric_df=pd.read_csv(\"../results/metrics/scoring_metrics.csv\")\n",
    "scoring_metric_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b52bfd-07b7-43b9-b743-01aabd24775c",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017df7dc-d874-4fb2-af05-8e070dd819c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "Optimizing hyperparameters in SVC with a smaller sample size of 10,000 instances is a strategy aimed at enhancing computational efficiency. This approach expedites the exploration of hyperparameter possibilities, aiding in the discovery of potential configurations. While the outcomes validate the concept, it's crucial to recognize and manage the constraints stemming from the smaller dataset size when interpreting the results.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9418a122",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "Best_Hyperparameters  = pd.read_csv(\"../results/metrics/best_params.csv\")\n",
    "bh = Best_Hyperparameters[['svc__C', 'svc__gamma', 'svc__kernel']]\n",
    "bh = bh.style.format().hide()\n",
    "glue(\"bh\", bh, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75037265-d1f2-4174-9f09-f908c4e5a7d4",
   "metadata": {},
   "source": [
    "```{glue:figure} bh\n",
    ":figwidth: 300px\n",
    ":name: \"Best Hyperparameters\"\n",
    "\n",
    "Best C, gamma and kernel parameters for svc_balanced model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d7db21-2c55-4297-a78e-7524d49fc64f",
   "metadata": {},
   "source": [
    "Random tested the SVC model with C values ranging from 0.1 to 10, gamma values ranging from 0.001 to 0.1, and kernels of rbf, sigmoid, and linear to Random tested the SVC model with C values ranging from 0.1 to 10, gamma values ranging from 0.001 to 0.1, and kernels of rbf, sigmoid, and linear to optimize the model's performance. With 25 random combinations with 5 folds of cross-validation, the best hyperparameter combination is approximately 4.33 for C, and approximately 0.01 for gamma, with the rbf kernel.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6db8d0-61a2-461d-b8cf-9f1d2fda19b7",
   "metadata": {},
   "source": [
    "### Test results after hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa64f7e7",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "accuracyNrecall = pd.read_csv(\"../results/metrics/model_scores.csv\")\n",
    "aNr = accuracyNrecall[['Accuracy', 'Recall']]\n",
    "aNr = aNr.style.format().hide()\n",
    "glue(\"aNr\", aNr, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc9ba4f-d49b-482c-b6a9-76ea7ff60743",
   "metadata": {},
   "source": [
    "```{glue:figure} aNr\n",
    ":figwidth: 300px\n",
    ":name: \"Test Results\"\n",
    "\n",
    "Accuracy and recall metrics on test data\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4384f7-0e95-4f79-bceb-055ae1f1df60",
   "metadata": {},
   "source": [
    "After fitting the model with the training data, and optimizing it with the hyperparameters found above, the model is used to score on the test data. The accuracy of the model is 0.86 while the recall ( True Positive / Actual Positive ) is 0.88. With optimization, the model performed well on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22df689c-62be-4c95-86a9-2e82533edb52",
   "metadata": {},
   "source": [
    "## Discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a76f22-f7f8-4edd-870b-abd1e59dd7e4",
   "metadata": {},
   "source": [
    "### Key Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ace47a8-cebe-4470-a797-2e5d959855e3",
   "metadata": {},
   "source": [
    "In this bank marketing analysis project, we aimed to develop a binary classification model to predict client subscription to term deposits. We tested Logistic Regression and Support Vector Classifier (SVC) models, focusing on recall as a key performance metric. The SVC model outperformed Logistic Regression in recall, and after hyperparameter optimization, it achieved a recall score of 0.875 on the test dataset, which is quite promising!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aaa939-7934-43db-8cdc-7921addbd44c",
   "metadata": {},
   "source": [
    "### Reflection on Expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87f4c4-7555-4393-bbcd-065cfd433668",
   "metadata": {},
   "source": [
    "The results were somewhat expected, given SVC's known efficacy in classification tasks, particularly when there's a clear margin of separation. The high recall score of 0.875 indicates that the model is particularly adept at identifying clients likely to subscribe, which was the primary goal. It's noteworthy that such a high recall was achieved, as it suggests the model is highly sensitive to true positive cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b897f2-0271-4d04-ae7b-e4ff0bf99ab3",
   "metadata": {},
   "source": [
    "### Impact of Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c65dd6-5f02-4742-853f-d53397dc7819",
   "metadata": {},
   "source": [
    "The high recall score of this model has significant implications for targeted marketing strategies. It suggests that the bank can confidently use the model's predictions to focus its marketing efforts on clients predicted to subscribe, potentially increasing the efficiency and effectiveness of its campaigns {cite}`moura2020optimization`. This targeted approach could lead to higher conversion rates with lower marketing expenses. However, it's important to balance such a high recall with precision to ensure that the bank doesn't unnecessarily target unlikely prospects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6af611-4a94-4b9f-a1fc-6beec5f8a08e",
   "metadata": {},
   "source": [
    "### Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b96100f-1bcb-45b9-8ef2-5c1433a84825",
   "metadata": {},
   "source": [
    "The success of this model leads to several potential areas for further exploration:\n",
    "\n",
    "- Balancing Precision and Recall: Investigating methods to enhance precision without substantially reducing recall.\n",
    "- Feature Analysis: Identifying which features most significantly influence subscription predictions.\n",
    "Model Interpretability: Improving the model's interpretability to better understand the basis for its predictions.\n",
    "- Temporal Adaptability: Assessing the model's adaptability to evolving trends and customer behaviors over time.\n",
    "- Testing Alternative Models: Exploring whether ensemble methods or more advanced machine learning algorithms could yield better or comparable results.\n",
    "- Customer Segmentation: Evaluating the model's performance across different customer segments to tailor more specific marketing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b6e27-a5e9-4a01-84c4-9aea31a097a7",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deedd3a-3929-4361-a214-db997820801b",
   "metadata": {},
   "source": [
    "```{bibliography}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}